{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.0.0-bin-hadoop2.7\\\\'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark, pyspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"OneHotEncoder\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| Id| Rating|\n",
      "+---+-------+\n",
      "|  0|   Good|\n",
      "|  1|    Bad|\n",
      "|  2|Average|\n",
      "|  3|   Good|\n",
      "|  4|    Bad|\n",
      "|  5|Average|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.createDataFrame(\n",
    "    [\n",
    "        (0, \"Good\"),\n",
    "        (1, \"Bad\"),\n",
    "        (2, \"Average\"),\n",
    "        (3, \"Good\"),\n",
    "        (4, \"Bad\"),\n",
    "        (5, \"Average\"),\n",
    "    ], ['Id', \"Rating\"]\n",
    ")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+-----+\n",
      "|_c0|   TV|radio|newspaper|sales|\n",
      "+---+-----+-----+---------+-----+\n",
      "|  1|230.1| 37.8|     69.2| 22.1|\n",
      "|  2| 44.5| 39.3|     45.1| 10.4|\n",
      "|  3| 17.2| 45.9|     69.3|  9.3|\n",
      "|  4|151.5| 41.3|     58.5| 18.5|\n",
      "|  5|180.8| 10.8|     58.4| 12.9|\n",
      "|  6|  8.7| 48.9|       75|  7.2|\n",
      "|  7| 57.5| 32.8|     23.5| 11.8|\n",
      "|  8|120.2| 19.6|     11.6| 13.2|\n",
      "|  9|  8.6|  2.1|        1|  4.8|\n",
      "| 10|199.8|  2.6|     21.2| 10.6|\n",
      "| 11| 66.1|  5.8|     24.2|  8.6|\n",
      "| 12|214.7|   24|        4| 17.4|\n",
      "| 13| 23.8| 35.1|     65.9|  9.2|\n",
      "| 14| 97.5|  7.6|      7.2|  9.7|\n",
      "| 15|204.1| 32.9|       46|   19|\n",
      "| 16|195.4| 47.7|     52.9| 22.4|\n",
      "| 17| 67.8| 36.6|      114| 12.5|\n",
      "| 18|281.4| 39.6|     55.8| 24.4|\n",
      "| 19| 69.2| 20.5|     18.3| 11.3|\n",
      "| 20|147.3| 23.9|     19.1| 14.6|\n",
      "+---+-----+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"../Advertising.csv\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', 'TV', 'radio', 'newspaper', 'sales']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- TV: string (nullable = true)\n",
      " |-- radio: string (nullable = true)\n",
      " |-- newspaper: string (nullable = true)\n",
      " |-- sales: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: float, TV: float, radio: float, newspaper: float, sales: float]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "new_data = df.select(*(col(c).cast(\"float\").alias(c) for c in df.columns))\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: float (nullable = true)\n",
      " |-- TV: float (nullable = true)\n",
      " |-- radio: float (nullable = true)\n",
      " |-- newspaper: float (nullable = true)\n",
      " |-- sales: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'CAST(_c0 AS FLOAT) AS `_c0`'>,\n",
       " Column<b'CAST(TV AS FLOAT) AS `TV`'>,\n",
       " Column<b'CAST(radio AS FLOAT) AS `radio`'>,\n",
       " Column<b'CAST(newspaper AS FLOAT) AS `newspaper`'>,\n",
       " Column<b'CAST(sales AS FLOAT) AS `sales`'>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list((col(c).cast(\"float\").alias(c) for c in df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+---------+-----+\n",
      "| _c0|   TV|radio|newspaper|sales|\n",
      "+----+-----+-----+---------+-----+\n",
      "| 1.0|230.1| 37.8|     69.2| 22.1|\n",
      "| 2.0| 44.5| 39.3|     45.1| 10.4|\n",
      "| 3.0| 17.2| 45.9|     69.3|  9.3|\n",
      "| 4.0|151.5| 41.3|     58.5| 18.5|\n",
      "| 5.0|180.8| 10.8|     58.4| 12.9|\n",
      "| 6.0|  8.7| 48.9|     75.0|  7.2|\n",
      "| 7.0| 57.5| 32.8|     23.5| 11.8|\n",
      "| 8.0|120.2| 19.6|     11.6| 13.2|\n",
      "| 9.0|  8.6|  2.1|      1.0|  4.8|\n",
      "|10.0|199.8|  2.6|     21.2| 10.6|\n",
      "|11.0| 66.1|  5.8|     24.2|  8.6|\n",
      "|12.0|214.7| 24.0|      4.0| 17.4|\n",
      "|13.0| 23.8| 35.1|     65.9|  9.2|\n",
      "|14.0| 97.5|  7.6|      7.2|  9.7|\n",
      "|15.0|204.1| 32.9|     46.0| 19.0|\n",
      "|16.0|195.4| 47.7|     52.9| 22.4|\n",
      "|17.0| 67.8| 36.6|    114.0| 12.5|\n",
      "|18.0|281.4| 39.6|     55.8| 24.4|\n",
      "|19.0| 69.2| 20.5|     18.3| 11.3|\n",
      "|20.0|147.3| 23.9|     19.1| 14.6|\n",
      "+----+-----+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Collection\n",
    "from pyspark.sql.functions import col, isnan, when, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---------+-----+\n",
      "|_c0| TV|radio|newspaper|sales|\n",
      "+---+---+-----+---------+-----+\n",
      "|  0|  0|    0|        0|    0|\n",
      "+---+---+-----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.select([count(when(col(c).isNull(), c)).alias(c) for c in new_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imputer_b0d01071404a"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=['TV', 'radio', 'newspaper'],\n",
    "    outputCols=['TV', 'radio', 'newspaper'],\n",
    ")\n",
    "imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImputerModel: uid=Imputer_b0d01071404a, strategy=mean, missingValue=NaN, numInputCols=3, numOutputCols=3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = imputer.fit(new_data)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: float, TV: float, radio: float, newspaper: float, sales: float]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_data = model.transform(new_data)\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---------+-----+\n",
      "|_c0| TV|radio|newspaper|sales|\n",
      "+---+---+-----+---------+-----+\n",
      "|  0|  0|    0|        0|    0|\n",
      "+---+---+-----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Checking for NaN type values in columns\n",
    "imputed_data.select([count(when(col(c).isNull(), c)).alias(c) for c in imputed_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: float, radio: float, newspaper: float, sales: float]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = imputed_data.drop('TV')\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+---------+-----+\n",
      "| _c0|   TV|radio|newspaper|sales|\n",
      "+----+-----+-----+---------+-----+\n",
      "| 1.0|230.1| 37.8|     69.2| 22.1|\n",
      "| 2.0| 44.5| 39.3|     45.1| 10.4|\n",
      "| 3.0| 17.2| 45.9|     69.3|  9.3|\n",
      "| 4.0|151.5| 41.3|     58.5| 18.5|\n",
      "| 5.0|180.8| 10.8|     58.4| 12.9|\n",
      "| 6.0|  8.7| 48.9|     75.0|  7.2|\n",
      "| 7.0| 57.5| 32.8|     23.5| 11.8|\n",
      "| 8.0|120.2| 19.6|     11.6| 13.2|\n",
      "| 9.0|  8.6|  2.1|      1.0|  4.8|\n",
      "|10.0|199.8|  2.6|     21.2| 10.6|\n",
      "|11.0| 66.1|  5.8|     24.2|  8.6|\n",
      "|12.0|214.7| 24.0|      4.0| 17.4|\n",
      "|13.0| 23.8| 35.1|     65.9|  9.2|\n",
      "|14.0| 97.5|  7.6|      7.2|  9.7|\n",
      "|15.0|204.1| 32.9|     46.0| 19.0|\n",
      "|16.0|195.4| 47.7|     52.9| 22.4|\n",
      "|17.0| 67.8| 36.6|    114.0| 12.5|\n",
      "|18.0|281.4| 39.6|     55.8| 24.4|\n",
      "|19.0| 69.2| 20.5|     18.3| 11.3|\n",
      "|20.0|147.3| 23.9|     19.1| 14.6|\n",
      "+----+-----+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_6d419a380400"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features.columns,\n",
    "    outputCol='Features'\n",
    ")\n",
    "assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Features: vector, TV: float]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = assembler.transform(imputed_data)\n",
    "output = output.select(\"Features\", \"TV\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            Features|   TV|\n",
      "+--------------------+-----+\n",
      "|[2.0,39.299999237...| 44.5|\n",
      "|[3.0,45.900001525...| 17.2|\n",
      "|[4.0,41.299999237...|151.5|\n",
      "|[5.0,10.800000190...|180.8|\n",
      "|[6.0,48.900001525...|  8.7|\n",
      "|[7.0,32.799999237...| 57.5|\n",
      "|[8.0,19.600000381...|120.2|\n",
      "|[9.0,2.0999999046...|  8.6|\n",
      "|[11.0,5.800000190...| 66.1|\n",
      "|[13.0,35.09999847...| 23.8|\n",
      "|[15.0,32.90000152...|204.1|\n",
      "|[17.0,36.59999847...| 67.8|\n",
      "|[18.0,39.59999847...|281.4|\n",
      "|[20.0,23.89999961...|147.3|\n",
      "|[21.0,27.70000076...|218.4|\n",
      "|[22.0,5.099999904...|237.4|\n",
      "|[25.0,12.60000038...| 62.3|\n",
      "|[26.0,3.5,19.5,12.0]|262.9|\n",
      "|[27.0,29.29999923...|142.9|\n",
      "|[28.0,16.70000076...|240.1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            Features|   TV|\n",
      "+--------------------+-----+\n",
      "|[1.0,37.799999237...|230.1|\n",
      "|[10.0,2.599999904...|199.8|\n",
      "|[12.0,24.0,4.0,17...|214.7|\n",
      "|[14.0,7.599999904...| 97.5|\n",
      "|[16.0,47.70000076...|195.4|\n",
      "|[19.0,20.5,18.299...| 69.2|\n",
      "|[23.0,15.89999961...| 13.2|\n",
      "|[24.0,16.89999961...|228.3|\n",
      "|[35.0,1.399999976...| 95.7|\n",
      "|[39.0,26.70000076...| 43.1|\n",
      "|[43.0,27.70000076...|293.6|\n",
      "|[44.0,8.399999618...|206.9|\n",
      "|[47.0,9.899999618...| 89.7|\n",
      "|[48.0,41.5,18.5,2...|239.9|\n",
      "|[52.0,9.600000381...|100.4|\n",
      "|[53.0,41.70000076...|216.4|\n",
      "|[56.0,49.40000152...|198.9|\n",
      "|[59.0,49.59999847...|210.8|\n",
      "|[63.0,15.5,27.299...|239.3|\n",
      "|[68.0,14.5,10.199...|139.3|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = output.randomSplit([0.7,0.3])\n",
    "train_df.show()\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lin_reg = LinearRegression(featuresCol=\"Features\", labelCol=\"TV\")\n",
    "linear_model= lin_reg.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.007915034610343038,-3.4152020025899787,0.11360930428435628,18.485926203263432]\n",
      "Intercepts: -35.946419096341515\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients:\", str(linear_model.coefficients))\n",
    "print(\"Intercepts:\", str(linear_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  34.942352626383936\n",
      "R2:  0.8386002929969353\n"
     ]
    }
   ],
   "source": [
    "trainSummary = linear_model.summary\n",
    "print(\"RMSE: \", trainSummary.rootMeanSquaredError)\n",
    "print(\"R2: \", trainSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+--------------------+\n",
      "|        prediction|   TV|            Features|\n",
      "+------------------+-----+--------------------+\n",
      "| 251.3676024996724|230.1|[1.0,37.799999237...|\n",
      "|153.61254851264596|199.8|[10.0,2.599999904...|\n",
      "|204.29325935892302|214.7|[12.0,24.0,4.0,17...|\n",
      "|118.34032410914128| 97.5|[14.0,7.599999904...|\n",
      "| 221.3697555995658|195.4|[16.0,47.70000076...|\n",
      "|105.16234531267452| 69.2|[19.0,20.5,18.299...|\n",
      "|19.090122455781824| 13.2|[23.0,15.89999961...|\n",
      "|196.03504920284257|228.3|[24.0,16.89999961...|\n",
      "|136.00633218636074| 95.7|[35.0,1.399999976...|\n",
      "|63.871919290525824| 43.1|[39.0,26.70000076...|\n",
      "| 252.6560125680585|293.6|[43.0,27.70000076...|\n",
      "| 177.1818734675995|206.9|[44.0,8.399999618...|\n",
      "| 130.6217660635421| 89.7|[47.0,9.899999618...|\n",
      "|253.67789360608543|239.9|[48.0,41.5,18.5,2...|\n",
      "| 129.8876225093335|100.4|[52.0,9.600000381...|\n",
      "|244.34001914629346|216.4|[53.0,41.70000076...|\n",
      "|240.71886208075523|198.9|[56.0,49.40000152...|\n",
      "|239.37465422061496|210.8|[59.0,49.59999847...|\n",
      "|204.94716882957658|239.3|[63.0,15.5,27.299...|\n",
      "| 163.9415931735471|139.3|[68.0,14.5,10.199...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Prediction\n",
    "predictions = linear_model.transform(test_df)\n",
    "predictions.select(\"prediction\", \"TV\", \"Features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Features: vector]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.select(\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            Features|   TV|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[1.0,37.799999237...|230.1| 251.3676024996724|\n",
      "|[10.0,2.599999904...|199.8|153.61254851264596|\n",
      "|[12.0,24.0,4.0,17...|214.7|204.29325935892302|\n",
      "|[14.0,7.599999904...| 97.5|118.34032410914128|\n",
      "|[16.0,47.70000076...|195.4| 221.3697555995658|\n",
      "|[19.0,20.5,18.299...| 69.2|105.16234531267452|\n",
      "|[23.0,15.89999961...| 13.2|19.090122455781824|\n",
      "|[24.0,16.89999961...|228.3|196.03504920284257|\n",
      "|[35.0,1.399999976...| 95.7|136.00633218636074|\n",
      "|[39.0,26.70000076...| 43.1|63.871919290525824|\n",
      "|[43.0,27.70000076...|293.6| 252.6560125680585|\n",
      "|[44.0,8.399999618...|206.9| 177.1818734675995|\n",
      "|[47.0,9.899999618...| 89.7| 130.6217660635421|\n",
      "|[48.0,41.5,18.5,2...|239.9|253.67789360608543|\n",
      "|[52.0,9.600000381...|100.4| 129.8876225093335|\n",
      "|[53.0,41.70000076...|216.4|244.34001914629346|\n",
      "|[56.0,49.40000152...|198.9|240.71886208075523|\n",
      "|[59.0,49.59999847...|210.8|239.37465422061496|\n",
      "|[63.0,15.5,27.299...|239.3|204.94716882957658|\n",
      "|[68.0,14.5,10.199...|139.3| 163.9415931735471|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = linear_model.transform(test_df)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
